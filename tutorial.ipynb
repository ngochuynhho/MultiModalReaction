{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f68747-6d5d-45c2-ba4f-3bafb76aa234",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9453fc-05fd-4989-ae0a-b270f5af4c27",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python\n",
    "!pip install librosa\n",
    "!pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e3688b3-f9f9-490b-ad47-aa12eeec74cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7e97b41-0921-4833-82ac-05916bedd5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from typing import List\n",
    "\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from PIL import Image\n",
    "from moviepy.editor import VideoFileClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60152578-1068-4869-99ed-bd7986b1f3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "## Limit memory grow\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth to True for each GPU\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76fa0ff-336e-4e39-9a90-57505a6b3ff0",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666b37d1-a4bd-4a0c-993e-a988c287c519",
   "metadata": {},
   "source": [
    "#### 1. Extracting visual and audio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8db89e-d20a-4863-886e-40195f872072",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file = 'dia0_utt0.mp4'\n",
    "visual_folder = 'visual_frames'\n",
    "os.makedirs(visual_folder, exist_ok=True)\n",
    "\n",
    "clip = VideoFileClip(video_file)\n",
    "frames = [frame for frame in clip.iter_frames()]\n",
    "for i, frame in enumerate(frames):\n",
    "    image = Image.fromarray(frame)\n",
    "    image.save(os.path.join(visual_folder, f'frame_{i:05d}.jpg'))\n",
    "\n",
    "audio = clip.audio\n",
    "audio.write_audiofile('dia0_utt0_audio.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7e8541-67a9-42db-9159-95dd7ffc5edb",
   "metadata": {},
   "source": [
    "#### 2. Extract face from frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f02e2b-2690-4a2c-9d0f-3851656a2e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_folder = 'face_frames'\n",
    "os.makedirs(face_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a54254-a7b7-4d2f-a271-54436d3204c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelFile = \"models/res10_300x300_ssd_iter_140000_fp16.caffemodel\"\n",
    "configFile = \"models/deploy.prototxt\"\n",
    "net = cv2.dnn.readNetFromCaffe(configFile, modelFile)\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd2b661-599f-4e6e-b529-2c2032b9ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_frames = glob.glob('visual_frames/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae4a524-53a0-4119-bad3-dc15cf7950c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in lst_frames:\n",
    "    bn = os.path.splitext(os.path.basename(f))[0]\n",
    "    frameOrig = cv2.imread(f)\n",
    "    frame = cv2.resize(frameOrig, (640, 480))\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), (104.0, 177.0, 123.0), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > 0.2:\n",
    "            box = detections[0, 0, i, 3:7] * np.array([frame.shape[1], frame.shape[0], frame.shape[1], frame.shape[0]])\n",
    "            (start_x, start_y, end_x, end_y) = box.astype(int)\n",
    "\n",
    "            # Extract the face region from the frame\n",
    "            face = frame[start_y:end_y, start_x:end_x]\n",
    "\n",
    "            # Save the face as a JPG image\n",
    "            face_filename = f'{face_folder}/face_{bn}.jpg'\n",
    "            cv2.imwrite(face_filename, face)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77dbdae-fa09-4aca-9beb-60faca59778f",
   "metadata": {},
   "source": [
    "#### 3. Transform audio waveform to log-mel spectrograme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1af4df2-2d0f-4815-af42-91384c4cf64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = 'dia0_utt0_audio.wav'\n",
    "y, sr = librosa.load(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f3f784-b9e2-4303-9778-9750f69468b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mel spectrogram\n",
    "mel_spec = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "# Convert to log scale (dB)\n",
    "log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852eb74a-4f0a-45d8-8139-71f01d43991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the log-mel spectrogram\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "#plt.colorbar(format='%+2.0f dB')\n",
    "#plt.title('Log-Mel Spectrogram')\n",
    "ax.set_axis_off()\n",
    "fig.add_axes(ax)\n",
    "librosa.display.specshow(log_mel_spec, sr=sr, x_axis='time', y_axis='mel')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b421c7ee-39d9-4e10-a1b6-568d34a0a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image = 'dia0_utt0_spectrogram_tight.png'\n",
    "fig.savefig(output_image, dpi=300, bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0a95de-52e5-4800-a82f-a44eedc28528",
   "metadata": {},
   "source": [
    "#### 4. Randomly generate data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e12620e-e4a0-42c4-ada5-582dcf80b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_one(tensor):\n",
    "    sum_values = tf.reduce_sum(tensor, axis=-1, keepdims=True)\n",
    "    return tensor / sum_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e756427-815b-406e-907b-6ed3c206d2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vis = tf.random.uniform(shape=(1000,500,1024))\n",
    "x_val_vis   = tf.random.uniform(shape=(100,500,1024))\n",
    "x_train_aud = tf.random.uniform(shape=(1000,500,512))\n",
    "x_val_aud   = tf.random.uniform(shape=(100,500,512))\n",
    "\n",
    "y_train_au  = tf.cast(tf.random.uniform(shape=(1000,500,10), minval=0, maxval=1)>0.5, dtype=tf.int32)\n",
    "y_val_au    = tf.cast(tf.random.uniform(shape=(100,500,10), minval=0, maxval=1)>0.5, dtype=tf.int32)\n",
    "y_train_emo = tf.cast(tf.random.uniform(shape=(1000,500,8), minval=0, maxval=1), dtype=tf.float32)\n",
    "y_val_emo   = tf.cast(tf.random.uniform(shape=(100,500,8), minval=0, maxval=1), dtype=tf.float32)\n",
    "y_train_emo = scale_one(y_train_emo)\n",
    "y_val_emo   = scale_one(y_val_emo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c88c0ab3-5a1f-421e-a4fa-c6c7369a79b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train_vis, x_train_aud, y_train_au, y_train_emo)).shuffle(1000).batch(32)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_val_vis, x_val_aud, y_val_au, y_val_emo)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c864bd83-cda9-4436-9523-c8c7605d7b3c",
   "metadata": {},
   "source": [
    "# Build Temporal Convolutional Network\n",
    "![TCNs](https://cdn-images-1.medium.com/max/1000/1*1cK-UEWHGaZLM-4ITCeqdQ.png)\n",
    "\n",
    "This module is used to encode both visual and audio representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce7c66d2-76ed-412b-bf77-57805f306e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_power_of_two(num: int):\n",
    "    return num != 0 and ((num & (num - 1)) == 0)\n",
    "\n",
    "def adjust_dilations(dilations: list):\n",
    "    if all([is_power_of_two(i) for i in dilations]):\n",
    "        return dilations\n",
    "    else:\n",
    "        new_dilations = [2 ** i for i in dilations]\n",
    "        return new_dilations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd25dd2-4da5-492e-b019-c11ab2612ee5",
   "metadata": {},
   "source": [
    "#### 1. Build Residual Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a77e5caa-eda8-4e99-94a2-0f931a8d544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    \"\"\" Defines the residual block for the WaveNet TCN\n",
    "    Args:\n",
    "        nb_filters: The number of convolutional filters to use in this block\n",
    "        kernel_size: The size of the convolutional kernel\n",
    "        padding: The padding used in the convolutional layers, 'same' or 'causal'.\n",
    "        activation: The final activation used in o = Activation(x + F(x))\n",
    "        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "        kernel_initializer: Initializer for the kernel weights matrix (Conv1D).\n",
    "        use_batch_norm: Whether to use batch normalization in the residual layers or not.\n",
    "        use_layer_norm: Whether to use layer normalization in the residual layers or not.\n",
    "        use_weight_norm: Whether to use weight normalization in the residual layers or not.\n",
    "        kwargs: Any initializers for Layer class.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 dilation_rate: int,\n",
    "                 nb_filters: int,\n",
    "                 kernel_size: int,\n",
    "                 padding: str,\n",
    "                 activation: str = 'relu',\n",
    "                 dropout_rate: float = 0,\n",
    "                 kernel_initializer: str = 'he_normal',\n",
    "                 use_batch_norm: bool = False,\n",
    "                 use_layer_norm: bool = False,\n",
    "                 use_weight_norm: bool = False,\n",
    "                 **kwargs):\n",
    "        \n",
    "        self.dilation_rate = dilation_rate\n",
    "        self.nb_filters = nb_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        self.use_weight_norm = use_weight_norm\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.layers = []\n",
    "        self.shape_match_conv = None\n",
    "        self.res_output_shape = None\n",
    "        self.final_activation = None\n",
    "        \n",
    "        super(ResidualBlock, self).__init__(**kwargs)\n",
    "        \n",
    "    def _build_layer(self, layer):\n",
    "        \"\"\" Helper function for building layer\n",
    "        Args:\n",
    "            layer: Appends layer to internal layer list and builds it based on the current output\n",
    "                   shape of ResidualBlocK. Updates current output shape.\n",
    "        \"\"\"\n",
    "        self.layers.append(layer)\n",
    "        self.layers[-1].build(self.res_output_shape)\n",
    "        self.res_output_shape = self.layers[-1].compute_output_shape(self.res_output_shape)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "\n",
    "        with tf.keras.backend.name_scope(self.name):  # name scope used to make sure weights get unique names\n",
    "            self.layers = []\n",
    "            self.res_output_shape = input_shape\n",
    "\n",
    "            for k in range(2):  # dilated conv block.\n",
    "                name = 'conv1D_{}'.format(k)\n",
    "                with tf.keras.backend.name_scope(name):  # name scope used to make sure weights get unique names\n",
    "                    conv = tf.keras.layers.Conv1D(\n",
    "                        filters=self.nb_filters,\n",
    "                        kernel_size=self.kernel_size,\n",
    "                        dilation_rate=self.dilation_rate,\n",
    "                        padding=self.padding,\n",
    "                        name=name,\n",
    "                        kernel_initializer=self.kernel_initializer\n",
    "                    )\n",
    "                    if self.use_weight_norm:\n",
    "                        from layers import WeightNormalization\n",
    "                        # wrap it. WeightNormalization API is different than BatchNormalization or LayerNormalization.\n",
    "                        with tf.keras.backend.name_scope('norm_{}'.format(k)):\n",
    "                            conv = WeightNormalization(conv)\n",
    "                    self._build_layer(conv)\n",
    "\n",
    "                with tf.keras.backend.name_scope('norm_{}'.format(k)):\n",
    "                    if self.use_batch_norm:\n",
    "                        self._build_layer(BatchNormalization())\n",
    "                    elif self.use_layer_norm:\n",
    "                        self._build_layer(LayerNormalization())\n",
    "                    elif self.use_weight_norm:\n",
    "                        pass  # done above.\n",
    "\n",
    "                with tf.keras.backend.name_scope('act_and_dropout_{}'.format(k)):\n",
    "                    self._build_layer(tf.keras.layers.Activation(self.activation, name='Act_Conv1D_{}'.format(k)))\n",
    "                    self._build_layer(tf.keras.layers.SpatialDropout1D(rate=self.dropout_rate, name='SDropout_{}'.format(k)))\n",
    "\n",
    "            if self.nb_filters != input_shape[-1]:\n",
    "                # 1x1 conv to match the shapes (channel dimension).\n",
    "                name = 'matching_conv1D'\n",
    "                with tf.keras.backend.name_scope(name):\n",
    "                    # make and build this layer separately because it directly uses input_shape.\n",
    "                    # 1x1 conv.\n",
    "                    self.shape_match_conv = tf.keras.layers.Conv1D(\n",
    "                        filters=self.nb_filters,\n",
    "                        kernel_size=1,\n",
    "                        padding='same',\n",
    "                        name=name,\n",
    "                        kernel_initializer=self.kernel_initializer\n",
    "                    )\n",
    "            else:\n",
    "                name = 'matching_identity'\n",
    "                self.shape_match_conv = tf.keras.layers.Lambda(lambda x: x, name=name)\n",
    "\n",
    "            with tf.keras.backend.name_scope(name):\n",
    "                self.shape_match_conv.build(input_shape)\n",
    "                self.res_output_shape = self.shape_match_conv.compute_output_shape(input_shape)\n",
    "\n",
    "            self._build_layer(tf.keras.layers.Activation(self.activation, name='Act_Conv_Blocks'))\n",
    "            self.final_activation = tf.keras.layers.Activation(self.activation, name='Act_Res_Block')\n",
    "            self.final_activation.build(self.res_output_shape)  # probably isn't necessary\n",
    "\n",
    "            # this is done to force Keras to add the layers in the list to self._layers\n",
    "            for layer in self.layers:\n",
    "                self.__setattr__(layer.name, layer)\n",
    "            self.__setattr__(self.shape_match_conv.name, self.shape_match_conv)\n",
    "            self.__setattr__(self.final_activation.name, self.final_activation)\n",
    "\n",
    "            super(ResidualBlock, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: The previous layer in the model\n",
    "            training: boolean indicating whether the layer should behave in training mode or in inference mode\n",
    "        \n",
    "        Returns: A tuple where the first element is the residual model tensor, and the second\n",
    "                 is the skip connection tensor.\n",
    "        \"\"\"\n",
    "        x1 = inputs\n",
    "        for layer in self.layers:\n",
    "            training_flag = 'training' in dict(inspect.signature(layer.call).parameters)\n",
    "            x1 = layer(x1, training=training) if training_flag else layer(x1)\n",
    "        x2 = self.shape_match_conv(inputs)\n",
    "        x1_x2 = self.final_activation(tf.keras.layers.add([x2, x1], name='Add_Res'))\n",
    "        return [x1_x2, x1]\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [self.res_output_shape, self.res_output_shape]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59ce44e-5913-41d3-8ad9-c4fbff232230",
   "metadata": {},
   "source": [
    "#### 2. Build TCN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7ac61ee-1db9-4cdc-a13a-b21957248a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCN(tf.keras.layers.Layer):\n",
    "    \"\"\"Creates a TCN layer.\n",
    "        Input shape:\n",
    "            A 3D tensor with shape (batch_size, timesteps, input_dim).\n",
    "            \n",
    "        Args:\n",
    "            nb_filters: The number of filters to use in the convolutional layers. Can be a list.\n",
    "            kernel_size: The size of the kernel to use in each convolutional layer.\n",
    "            dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n",
    "            nb_stacks : The number of stacks of residual blocks to use.\n",
    "            padding: The padding to use in the convolutional layers, 'causal' or 'same'.\n",
    "            use_skip_connections: Boolean. If we want to add skip connections from input to each residual blocK.\n",
    "            return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n",
    "            activation: The activation used in the residual blocks o = Activation(x + F(x)).\n",
    "            dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "            kernel_initializer: Initializer for the kernel weights matrix (Conv1D).\n",
    "            use_batch_norm: Whether to use batch normalization in the residual layers or not.\n",
    "            use_layer_norm: Whether to use layer normalization in the residual layers or not.\n",
    "            use_weight_norm: Whether to use weight normalization in the residual layers or not.\n",
    "            go_backwards: Boolean (default False). If True, process the input sequence backwards and\n",
    "            return the reversed sequence.\n",
    "            return_state: Boolean. Whether to return the last state in addition to the output. Default: False.\n",
    "            kwargs: Any other arguments for configuring parent class Layer. For example \"name=str\", Name of the model.\n",
    "                    Use unique names when using multiple TCN.\n",
    "                    \n",
    "        Returns:\n",
    "            A TCN layer.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 nb_filters=64,\n",
    "                 kernel_size=3,\n",
    "                 nb_stacks=1,\n",
    "                 dilations=(1, 2, 4, 8, 16, 32),\n",
    "                 padding='causal',\n",
    "                 use_skip_connections=True,\n",
    "                 dropout_rate=0.0,\n",
    "                 return_sequences=True,\n",
    "                 activation='relu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 use_batch_norm=False,\n",
    "                 use_layer_norm=False,\n",
    "                 use_weight_norm=False,\n",
    "                 go_backwards=False,\n",
    "                 return_state=False,\n",
    "                 **kwargs):\n",
    "        self.return_sequences = return_sequences\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_skip_connections = use_skip_connections\n",
    "        self.dilations = dilations\n",
    "        self.nb_stacks = nb_stacks\n",
    "        self.kernel_size = kernel_size\n",
    "        self.nb_filters = nb_filters\n",
    "        self.activation_name = activation\n",
    "        self.padding = padding\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        self.use_weight_norm = use_weight_norm\n",
    "        self.go_backwards = go_backwards\n",
    "        self.return_state = return_state\n",
    "        self.skip_connections = []\n",
    "        self.residual_blocks = []\n",
    "        self.layers_outputs = []\n",
    "        self.build_output_shape = None\n",
    "        self.slicer_layer = None  # in case return_sequence=False\n",
    "        self.output_slice_index = None  # in case return_sequence=False\n",
    "        self.padding_same_and_time_dim_unknown = False  # edge case if padding='same' and time_dim = None\n",
    "        \n",
    "        if self.use_batch_norm + self.use_layer_norm + self.use_weight_norm > 1:\n",
    "            raise ValueError('Only one normalization can be specified at once.')\n",
    "\n",
    "        if isinstance(self.nb_filters, list):\n",
    "            assert len(self.nb_filters) == len(self.dilations)\n",
    "            if len(set(self.nb_filters)) > 1 and self.use_skip_connections:\n",
    "                raise ValueError('Skip connections are not compatible '\n",
    "                                 'with a list of filters, unless they are all equal.')\n",
    "\n",
    "        if padding != 'causal' and padding != 'same':\n",
    "            raise ValueError(\"Only 'causal' or 'same' padding are compatible for this layer.\")\n",
    "\n",
    "        # initialize parent class\n",
    "        super(TCN, self).__init__(**kwargs)\n",
    "        \n",
    "    @property\n",
    "    def receptive_field(self):\n",
    "        return 1 + 2 * (self.kernel_size - 1) * self.nb_stacks * sum(self.dilations)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "\n",
    "        # member to hold current output shape of the layer for building purposes\n",
    "        self.build_output_shape = input_shape\n",
    "\n",
    "        # list to hold all the member ResidualBlocks\n",
    "        self.residual_blocks = []\n",
    "        total_num_blocks = self.nb_stacks * len(self.dilations)\n",
    "        if not self.use_skip_connections:\n",
    "            total_num_blocks += 1  # cheap way to do a false case for below\n",
    "\n",
    "        for s in range(self.nb_stacks):\n",
    "            for i, d in enumerate(self.dilations):\n",
    "                res_block_filters = self.nb_filters[i] if isinstance(self.nb_filters, list) else self.nb_filters\n",
    "                self.residual_blocks.append(ResidualBlock(dilation_rate=d,\n",
    "                                                          nb_filters=res_block_filters,\n",
    "                                                          kernel_size=self.kernel_size,\n",
    "                                                          padding=self.padding,\n",
    "                                                          activation=self.activation_name,\n",
    "                                                          dropout_rate=self.dropout_rate,\n",
    "                                                          use_batch_norm=self.use_batch_norm,\n",
    "                                                          use_layer_norm=self.use_layer_norm,\n",
    "                                                          use_weight_norm=self.use_weight_norm,\n",
    "                                                          kernel_initializer=self.kernel_initializer,\n",
    "                                                          name='residual_block_{}'.format(len(self.residual_blocks))))\n",
    "                # build newest residual block\n",
    "                self.residual_blocks[-1].build(self.build_output_shape)\n",
    "                self.build_output_shape = self.residual_blocks[-1].res_output_shape\n",
    "\n",
    "        # this is done to force keras to add the layers in the list to self._layers\n",
    "        for layer in self.residual_blocks:\n",
    "            self.__setattr__(layer.name, layer)\n",
    "\n",
    "        self.output_slice_index = None\n",
    "        if self.padding == 'same':\n",
    "            time = self.build_output_shape.as_list()[1]\n",
    "            if time is not None:  # if time dimension is defined. e.g. shape = (bs, 500, input_dim).\n",
    "                self.output_slice_index = int(self.build_output_shape.as_list()[1] / 2)\n",
    "            else:\n",
    "                # It will known at call time. c.f. self.call.\n",
    "                self.padding_same_and_time_dim_unknown = True\n",
    "\n",
    "        else:\n",
    "            self.output_slice_index = -1  # causal case.\n",
    "        self.slicer_layer = tf.keras.layers.Lambda(lambda tt: tt[:, self.output_slice_index, :], name='Slice_Output')\n",
    "        self.slicer_layer.build(self.build_output_shape.as_list())\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if not self.built:\n",
    "            self.build(input_shape)\n",
    "        if not self.return_sequences:\n",
    "            batch_size = self.build_output_shape[0]\n",
    "            batch_size = batch_size.value if hasattr(batch_size, 'value') else batch_size\n",
    "            nb_filters = self.build_output_shape[-1]\n",
    "            return [batch_size, nb_filters]\n",
    "        else:\n",
    "            # Compatibility tensorflow 1.x\n",
    "            return [v.value if hasattr(v, 'value') else v for v in self.build_output_shape]\n",
    "        \n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        x = inputs\n",
    "\n",
    "        if self.go_backwards:\n",
    "            # reverse x in the time axis\n",
    "            x = tf.reverse(x, axis=[1])\n",
    "\n",
    "        self.layers_outputs = [x]\n",
    "        self.skip_connections = []\n",
    "        for res_block in self.residual_blocks:\n",
    "            try:\n",
    "                x, skip_out = res_block(x, training=training)\n",
    "            except TypeError:  # compatibility with tensorflow 1.x\n",
    "                x, skip_out = res_block(K.cast(x, 'float32'), training=training)\n",
    "            self.skip_connections.append(skip_out)\n",
    "            self.layers_outputs.append(x)\n",
    "\n",
    "        if self.use_skip_connections:\n",
    "            if len(self.skip_connections) > 1:\n",
    "                # Keras: A merge layer should be called on a list of at least 2 inputs. Got 1 input.\n",
    "                x = tf.keras.layers.add(self.skip_connections, name='Add_Skip_Connections')\n",
    "            else:\n",
    "                x = self.skip_connections[0]\n",
    "            self.layers_outputs.append(x)\n",
    "\n",
    "        if not self.return_sequences:\n",
    "            # case: time dimension is unknown. e.g. (bs, None, input_dim).\n",
    "            if self.padding_same_and_time_dim_unknown:\n",
    "                self.output_slice_index = K.shape(self.layers_outputs[-1])[1] // 2\n",
    "            x = self.slicer_layer(x)\n",
    "            self.layers_outputs.append(x)\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Returns the config of a the layer. This is used for saving and loading from a model\n",
    "        :return: python dictionary with specs to rebuild layer\n",
    "        \"\"\"\n",
    "        config = super(TCN, self).get_config()\n",
    "        config['nb_filters'] = self.nb_filters\n",
    "        config['kernel_size'] = self.kernel_size\n",
    "        config['nb_stacks'] = self.nb_stacks\n",
    "        config['dilations'] = self.dilations\n",
    "        config['padding'] = self.padding\n",
    "        config['use_skip_connections'] = self.use_skip_connections\n",
    "        config['dropout_rate'] = self.dropout_rate\n",
    "        config['return_sequences'] = self.return_sequences\n",
    "        config['activation'] = self.activation_name\n",
    "        config['use_batch_norm'] = self.use_batch_norm\n",
    "        config['use_layer_norm'] = self.use_layer_norm\n",
    "        config['use_weight_norm'] = self.use_weight_norm\n",
    "        config['kernel_initializer'] = self.kernel_initializer\n",
    "        config['go_backwards'] = self.go_backwards\n",
    "        config['return_state'] = self.return_state\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08d068c0-56eb-4dec-9cc3-cb204f2dced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, timesteps, projection_dim):\n",
    "        super().__init__()\n",
    "        self.timesteps  = timesteps\n",
    "        self.projection = tf.keras.layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=self.timesteps, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.timesteps, delta=1)\n",
    "        encoded   = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f033419b-fa5d-4ed1-bd82-082f29aa27c3",
   "metadata": {},
   "source": [
    "#### 3. Build Fusion module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2af619d4-4ad9-474e-9f6e-9395ae72a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCAN(tf.keras.layers.Layer):\n",
    "    \"\"\"Temporal Co-Attention Networks\n",
    "    Inputs:\n",
    "        list of modalities [M1, M2]\n",
    "        M1 has shape [batch_size, timestep, feature_dim_1]\n",
    "        M2 has shape [batch_size, timestep, feature_dim_2]\n",
    "    Args:\n",
    "        hidden_units: output dimension of feature channel to be returned\n",
    "    Output:\n",
    "        A fused features with shape [batch_size, timestep, 2*hidden_units]\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_units):\n",
    "        super(TCAN, self).__init__()\n",
    "        \n",
    "        self.hidden_units = hidden_units\n",
    "        \n",
    "        # Attention weights\n",
    "        self.attention_weights1 = tf.keras.layers.Dense(1)\n",
    "        self.attention_weights2 = tf.keras.layers.Dense(1)\n",
    "        \n",
    "        # Temporal co-attention\n",
    "        self.co_attention1 = tf.keras.layers.Dense(hidden_units)\n",
    "        self.co_attention2 = tf.keras.layers.Dense(hidden_units)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        modality1_features, modality2_features = inputs\n",
    "        \n",
    "        # Compute attention scores for modality 1\n",
    "        attention_scores1 = self.attention_weights1(modality1_features)\n",
    "        \n",
    "        # Compute attention scores for modality 2\n",
    "        attention_scores2 = self.attention_weights2(modality2_features)\n",
    "        \n",
    "        # Compute attention weights for modality 1\n",
    "        attention_weights1 = tf.nn.softmax(attention_scores1, axis=1)\n",
    "        \n",
    "        # Compute attention weights for modality 2\n",
    "        attention_weights2 = tf.nn.softmax(attention_scores2, axis=1)\n",
    "        \n",
    "        # Apply attention to modality 1\n",
    "        attended_modality1 = modality1_features * attention_weights1\n",
    "        \n",
    "        # Apply attention to modality 2\n",
    "        attended_modality2 = modality2_features * attention_weights2\n",
    "        \n",
    "        # Compute temporal co-attention\n",
    "        co_attention1 = self.co_attention1(attended_modality1)\n",
    "        co_attention2 = self.co_attention2(attended_modality2)\n",
    "        \n",
    "        # Concatenate co-attention features\n",
    "        co_attention_features = tf.concat([co_attention1, co_attention2], axis=-1)\n",
    "        \n",
    "        return co_attention_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8bdeceb-dbfe-402e-a6f4-cfcfc048700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTRN(tf.keras.layers.Layer):\n",
    "    \"\"\"Multimodal Temporal Relation Networks\n",
    "    Inputs:\n",
    "        list of modalities [M1, M2]\n",
    "        M1 has shape [batch_size, timestep, feature_dim_1]\n",
    "        M2 has shape [batch_size, timestep, feature_dim_2]\n",
    "    Args:\n",
    "        hidden_units: output dimension of feature channel to be returned\n",
    "    Output:\n",
    "        A fused features with shape [batch_size, timestep, hidden_units]\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_units):\n",
    "        super(MTRN, self).__init__()\n",
    "\n",
    "        self.hidden_units = hidden_units\n",
    "\n",
    "        # Temporal relation modeling\n",
    "        self.temporal_relation1 = tf.keras.layers.Dense(hidden_units)\n",
    "        self.temporal_relation2 = tf.keras.layers.Dense(hidden_units)\n",
    "        self.temporal_relation3 = tf.keras.layers.Dense(hidden_units)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        modality1_features, modality2_features = inputs\n",
    "\n",
    "        # Compute temporal relations\n",
    "        relation1 = self.temporal_relation1(modality1_features)\n",
    "        relation2 = self.temporal_relation2(modality2_features)\n",
    "        relation3 = tf.multiply(relation1, relation2)\n",
    "\n",
    "        # Fusion with element-wise sum\n",
    "        fused_output = tf.add(tf.add(relation1, relation2), relation3)\n",
    "\n",
    "        return fused_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa80edc-d759-4231-923b-8d91ba7de52a",
   "metadata": {},
   "source": [
    "#### 4. Build full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b21a8298-3d4e-41ad-aa15-945113245b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalEmotion(tf.keras.Model):\n",
    "    \"\"\" Multimodal audio-visual for single task\n",
    "    Args:\n",
    "        timesteps: number of timesteps\n",
    "        output_dim: the output size of prediction\n",
    "        tcn_config_model1: set of configuration for the first model \n",
    "        tcn_config_model2: set of configuration for the second model\n",
    "        fused_mode: type of fision technique\n",
    "        fused_units: dimension size of the fusion output\n",
    "        output_activation: activation function in text for output prediction\n",
    "        use_pe: whether to use ppositional encoding or not\n",
    "\n",
    "    Return:\n",
    "        tensorflow model\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 timesteps,\n",
    "                 output_dim,\n",
    "                 tcn_config_model1, # config of TCN for visual \n",
    "                 tcn_config_model2, # config of TCn for audio\n",
    "                 fused_mode=None, # if None, simplify concatenate\n",
    "                 fused_units = 32,\n",
    "                 output_activation='softmax',\n",
    "                 use_pe=False,\n",
    "                 **kwargs):\n",
    "        # initialize parent class\n",
    "        super(MultimodalEmotion, self).__init__(**kwargs)\n",
    "\n",
    "        self.timesteps   = timesteps\n",
    "        self.output_dim  = output_dim\n",
    "        self.output_activation = output_activation\n",
    "        self.fused_mode  = fused_mode\n",
    "        self.fused_units = fused_units\n",
    "        self.tcn_config_model1 = tcn_config_model1\n",
    "        self.tcn_config_model2 = tcn_config_model2\n",
    "        self.use_pe = use_pe\n",
    "        # Encoding layers\n",
    "        self.tcn_model1 = TCN(**tcn_config_model1, name='tcn_model_1')\n",
    "        self.tcn_model2 = TCN(**tcn_config_model2, name='tcn_model_2')\n",
    "        # Fusion layer\n",
    "        if self.fused_mode=='TCAN':\n",
    "            self.fused_layer = TCAN(self.fused_units)\n",
    "        elif self.fused_mode=='MTRN':\n",
    "            self.fused_layer = MTRN(self.fused_units)\n",
    "        #else:\n",
    "        #    self.fused_layer = tf.keras.layers.Concatenate(axis=-1)\n",
    "        # Add positional Encoding\n",
    "        if self.use_pe:\n",
    "            self.pe_layer = PositionalEncoder(self.timesteps, self.fused_units)\n",
    "        # Output layer\n",
    "        self.output_layer = tf.keras.layers.Dense(self.output_dim)\n",
    "        \n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        x_1, x_2 = inputs\n",
    "        \n",
    "        x_1 = self.tcn_model1(x_1, training=training)\n",
    "        x_2 = self.tcn_model2(x_2, training=training)\n",
    "        \n",
    "        x_fused = self.fused_layer([x_1, x_2])\n",
    "\n",
    "        if self.use_pe:\n",
    "            x_fused   = self.pe_layer(x_fused)\n",
    "\n",
    "        x_out = self.output_layer(x_fused)\n",
    "        if self.output_activation=='softmax':\n",
    "            x_out = tf.nn.softmax(x_out)\n",
    "        elif self.output_activation=='tanh':\n",
    "            x_out = tf.keras.activations.tanh(x_out)\n",
    "\n",
    "        return x_out\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MultimodalEmotion, self).get_config()\n",
    "        config['timesteps']         = self.timesteps\n",
    "        config['output_dim']        = self.output_dim\n",
    "        config['output_activation'] = self.output_activation\n",
    "        config['tcn_config_model1'] = self.tcn_config_model1\n",
    "        config['tcn_config_model2'] = self.tcn_config_model2\n",
    "        config['fused_mode']        = self.fused_mode\n",
    "        config['fused_units']       = self.fused_units\n",
    "        config['use_pe']            = self.use_pe\n",
    "        return config  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f1fdafc-ac51-4d6e-9ef1-e33000c34153",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcn_config = {\n",
    "    'nb_filters':256,\n",
    "    'kernel_size':3,\n",
    "    'nb_stacks':1,\n",
    "    'dilations':(1, 2, 4, 8, 16, 32),\n",
    "    'padding':'causal',\n",
    "    'use_skip_connections':True,\n",
    "    'dropout_rate':0.0,\n",
    "    'return_sequences':True,\n",
    "    'activation':'relu',\n",
    "    'kernel_initializer':'he_normal',\n",
    "    'use_batch_norm':False,\n",
    "    'use_layer_norm':False,\n",
    "    'use_weight_norm':False,\n",
    "    'go_backwards':False,\n",
    "    'return_state':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a7fe6aa-b12a-4e2c-ae56-7c08d5105a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multimodal model for AU prediction\n",
    "multi_model_au = MultimodalEmotion(\n",
    "                          timesteps=x_train_vis.shape[1],\n",
    "                          output_dim=10,\n",
    "                          output_activation='sigmoid',\n",
    "                          tcn_config_model1=tcn_config,\n",
    "                          tcn_config_model2=tcn_config,\n",
    "                          fused_mode='TCAN',\n",
    "                          fused_units=128,\n",
    "                          use_pe=True,\n",
    "                  )\n",
    "## Multimodal model for emotion recognisiotn\n",
    "multi_model_emo = MultimodalEmotion(\n",
    "                          timesteps=x_train_vis.shape[1],\n",
    "                          output_dim=8,\n",
    "                          output_activation='softmax',\n",
    "                          tcn_config_model1=tcn_config,\n",
    "                          tcn_config_model2=tcn_config,\n",
    "                          fused_mode='TCAN',\n",
    "                          fused_units=128,\n",
    "                          use_pe=True,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c35367-70dd-4a5a-877f-ce342a308881",
   "metadata": {},
   "source": [
    "# Training Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675bc0d6-6e41-4bd2-be15-2cba45804018",
   "metadata": {},
   "source": [
    "#### 1. Define loss function and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da17506b-6558-4493-82a4-87b92e6cf924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_loss(y_true, y_pred):\n",
    "    y_true_flat = tf.reshape(y_true, shape=(-1, y_true.shape[-1]))\n",
    "    y_pred_flat = tf.reshape(y_pred, shape=(-1, y_true.shape[-1]))\n",
    "\n",
    "    bce_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    loss = bce_loss(y_true_flat, y_pred_flat)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2cda8bd-a582-497c-b5a5-a1c9bd63c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MultiLabelMetrics(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='multi_label_metrics', **kwargs):\n",
    "        super(MultiLabelMetrics, self).__init__(name=name, **kwargs)\n",
    "        self.precision = self.add_weight(name='precision', initializer='zeros')\n",
    "        self.recall = self.add_weight(name='recall', initializer='zeros')\n",
    "        self.f1_score = self.add_weight(name='f1_score', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.bool)\n",
    "        y_pred = tf.cast(y_pred, tf.bool)\n",
    "\n",
    "        true_positives = tf.reduce_sum(tf.cast(tf.logical_and(y_true, y_pred), tf.float32), axis=(0, 1))\n",
    "        predicted_positives = tf.reduce_sum(tf.cast(y_pred, tf.float32), axis=(0, 1))\n",
    "        actual_positives = tf.reduce_sum(tf.cast(y_true, tf.float32), axis=(0, 1))\n",
    "\n",
    "        precision = true_positives / (predicted_positives + tf.keras.backend.epsilon())\n",
    "        recall = true_positives / (actual_positives + tf.keras.backend.epsilon())\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
    "\n",
    "        self.precision.assign_add(tf.reduce_mean(precision))\n",
    "        self.recall.assign_add(tf.reduce_mean(recall))\n",
    "        self.f1_score.assign_add(tf.reduce_mean(f1_score))\n",
    "\n",
    "    def result(self):\n",
    "        return self.precision, self.recall, self.f1_score\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.assign(0.0)\n",
    "        self.recall.assign(0.0)\n",
    "        self.f1_score.assign(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a02282-3861-4dbc-b202-c11f027a58b5",
   "metadata": {},
   "source": [
    "#### 2. Define training and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fe77ed6-c4f6-44e3-aa9f-4ec906b9afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-1\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfcaa2ae-60b0-4ba7-9435-2d3b5ced595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = MultiLabelMetrics(name='train_accuracy')\n",
    "\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "val_accuracy = MultiLabelMetrics(name='val_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e6004f7-cd9d-496c-8261-dbb6c9421277",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(xs, y_true):\n",
    "    \"\"\"\n",
    "    xs: list of input data\n",
    "    y_true: grouth truth\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = multi_model_au(xs, training=True)\n",
    "        loss   = multi_label_loss(y_true, y_pred)\n",
    "    gradients  = tape.gradient(loss, multi_model_au.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, multi_model_au.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy.update_state(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64e5eca1-8606-4570-906d-a620dd26e0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(xs, y_true):\n",
    "    \"\"\"\n",
    "    xs: list of input data\n",
    "    y_true: grouth truth\n",
    "    \"\"\"\n",
    "    y_pred = multi_model_au(xs, training=True)\n",
    "    loss   = multi_label_loss(y_true, y_pred)\n",
    "\n",
    "    val_loss(loss)\n",
    "    val_accuracy.update_state(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a219aec8-d9cc-47b8-9039-e2a242ddb3fe",
   "metadata": {},
   "source": [
    "#### 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44661bf9-7abf-44b8-881d-2ebb2136bb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.454263687133789, F1: 21.33376121520996, Validation Loss: 7.677295684814453, Validation F1: 2.6612868309020996\n",
      "Epoch 2, Loss: 7.685193061828613, F1: 21.330249786376953, Validation Loss: 7.677295684814453, Validation F1: 2.6612868309020996\n",
      "Epoch 3, Loss: 7.686321258544922, F1: 21.33146858215332, Validation Loss: 7.677295684814453, Validation F1: 2.6612868309020996\n",
      "Epoch 4, Loss: 7.685826301574707, F1: 21.331247329711914, Validation Loss: 7.677295684814453, Validation F1: 2.6612868309020996\n",
      "Epoch 5, Loss: 7.684256076812744, F1: 21.32970428466797, Validation Loss: 7.677295684814453, Validation F1: 2.6612868309020996\n",
      "Epoch 6, Loss: 7.684277057647705, F1: 21.331274032592773, Validation Loss: 7.677295684814453, Validation F1: 2.6612868309020996\n",
      "Epoch 7, Loss: 7.68722677230835, F1: 21.334400177001953, Validation Loss: 7.677295684814453, Validation F1: 2.6612868309020996\n",
      "Epoch 8, Loss: 7.685286521911621, F1: 21.331878662109375, Validation Loss: 7.677295684814453, Validation F1: 2.6612868309020996\n",
      "Epoch 9, Loss: 7.6839518547058105, F1: 21.331661224365234, Validation Loss: 7.677295684814453, Validation F1: 2.6612868309020996\n",
      "Epoch 10, Loss: 7.687133312225342, F1: 21.33206558227539, Validation Loss: 7.677295684814453, Validation F1: 2.6612868309020996\n",
      "Epoch 11, Loss: 7.6851091384887695, F1: 21.32892608642578, Validation Loss: 7.677295684814453, Validation F1: 2.6612868309020996\n",
      "Epoch 12, Loss: 7.685141563415527, F1: 21.33188247680664, Validation Loss: 7.677295684814453, Validation F1: 2.6612868309020996\n",
      "Epoch 13, Loss: 7.684418201446533, F1: 21.331127166748047, Validation Loss: 7.677295684814453, Validation F1: 2.6612868309020996\n",
      "Epoch 14, Loss: 7.685718059539795, F1: 21.33172035217285, Validation Loss: 7.677295684814453, Validation F1: 2.6612868309020996\n",
      "Epoch 15, Loss: 7.68766975402832, F1: 21.328657150268555, Validation Loss: 7.677295684814453, Validation F1: 2.6612868309020996\n",
      "Epoch 16, Loss: 7.686448574066162, F1: 21.3320369720459, Validation Loss: 7.677295684814453, Validation F1: 2.6612868309020996\n",
      "Epoch 17, Loss: 7.685389041900635, F1: 21.334461212158203, Validation Loss: 7.677295684814453, Validation F1: 2.6612868309020996\n",
      "Epoch 18, Loss: 7.686249732971191, F1: 21.33180809020996, Validation Loss: 7.677295684814453, Validation F1: 2.6612868309020996\n",
      "Epoch 19, Loss: 7.683863639831543, F1: 21.33258056640625, Validation Loss: 7.677295684814453, Validation F1: 2.6612868309020996\n",
      "Epoch 20, Loss: 7.682656764984131, F1: 21.333484649658203, Validation Loss: 7.677295684814453, Validation F1: 2.6612868309020996\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    val_accuracy.reset_states()\n",
    "\n",
    "    for xt_1, xt_2, yt_1, yt_2 in train_ds:\n",
    "        train_step([xt_1, xt_2], yt_1)\n",
    "\n",
    "    for xv_1, xv_2, yv_1, yv_2 in val_ds:\n",
    "        test_step([xv_1, xv_2], yv_1)\n",
    "\n",
    "    pre, rec, f1 = train_accuracy.result()\n",
    "    val_pre, val_rec, val_f1 = val_accuracy.result()\n",
    "    \n",
    "    print(\n",
    "      f'Epoch {epoch + 1}, '\n",
    "      f'Loss: {train_loss.result()}, '\n",
    "      f'F1: {f1.numpy()}, '\n",
    "      f'Validation Loss: {val_loss.result()}, '\n",
    "      f'Validation F1: {val_f1.numpy()}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54009d40-2a1a-48ed-b96b-2690610e0295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
